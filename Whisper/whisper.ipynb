{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAelx-6NpeH9"
      },
      "source": [
        "# WHISPER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFp9KbcQsjAc"
      },
      "source": [
        "`Whisper` es un modelo de reconocimiento de voz de propósito general. Está entrenado en un gran conjunto de datos de audio diverso (680.000 horas de audios transcritos) y también es un modelo multitarea que puede realizar reconocimiento de voz multilingüe, así como traducción de voz e identificación de idioma."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZI6SlBlqaOZ"
      },
      "source": [
        "## Acerca del modelo\n",
        "\n",
        "Whisper es un modelo de Transformer sequence-to-sequence entrenado en varias tareas de procesamiento de voz, incluido el reconocimiento de voz multilingüe, la traducción de voz, la identificación del idioma hablado y la detección de actividad de voz. Todas estas tareas se representan conjuntamente como una secuencia de tokens que el decodificador debe predecir, lo que permite que un solo modelo reemplace muchas etapas diferentes de una canalización de procesamiento de voz tradicional. El formato de entrenamiento multitarea utiliza un conjunto de tokens especiales que sirven como especificadores de tareas u objetivos de clasificación.\n",
        "\n",
        "<img src=\"https://github.com/openai/whisper/blob/main/approach.png?raw=true\"><br>\n",
        "\n",
        "En este [paper](https://cdn.openai.com/papers/whisper.pdf) se puede leer un poco mas de su desarrollo. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjKUPd57sYu_"
      },
      "source": [
        "## Configuración"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71DYR4aE9jY0"
      },
      "source": [
        "Primero, hay que asegurarse de estar usando un entorno de ejecución GPU para ejecutar este notebook, de modo que la inferencia sea mucho más rápida. Si el siguiente comando falla, use el menú `entorno de ejecución` del menu y seleccione `Cambiar tipo de entorno ejecución`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CefkXah99qD0",
        "outputId": "4f6766d2-f2ec-4b3b-f9fe-7c73ecf91148"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Nov 19 12:57:29 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1jziqe3skM1"
      },
      "source": [
        "Se uso Python 3.9.9 y PyTorch 1.10.1 para entrenar y probar el modelo, pero se espera que el código base sea compatible con Python 3.7 o versiones posteriores y recientes de PyTorch. El código base también depende de algunos paquetes de Python, sobre todo [HuggingFace Transformers](https://huggingface.co/docs/transformers/index) para su rápida implementación de tokenizer y [ffmpeg-python](https://github.com/kkroening/ffmpeg-python) para leer archivos de audio. El siguiente comando extraerá e instalará la última confirmación de este repositorio, junto con sus dependencias de Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0vso9TQpXwn",
        "outputId": "7158a605-19dd-4ff2-96f3-08e2d8aad2aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-fxlfepuj\n",
            "  Running command git clone -q https://github.com/openai/whisper.git /tmp/pip-req-build-fxlfepuj\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from whisper==1.0) (1.21.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from whisper==1.0) (1.12.1+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from whisper==1.0) (4.64.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from whisper==1.0) (9.0.0)\n",
            "Collecting transformers>=4.19.0\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 33.2 MB/s \n",
            "\u001b[?25hCollecting ffmpeg-python==0.2.0\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from ffmpeg-python==0.2.0->whisper==1.0) (0.16.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=4.19.0->whisper==1.0) (4.13.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.19.0->whisper==1.0) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 62.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.19.0->whisper==1.0) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.19.0->whisper==1.0) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>=4.19.0->whisper==1.0) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.19.0->whisper==1.0) (3.8.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 73.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers>=4.19.0->whisper==1.0) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=4.19.0->whisper==1.0) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=4.19.0->whisper==1.0) (3.10.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.19.0->whisper==1.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.19.0->whisper==1.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.19.0->whisper==1.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.19.0->whisper==1.0) (2022.9.24)\n",
            "Building wheels for collected packages: whisper\n",
            "  Building wheel for whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for whisper: filename=whisper-1.0-py3-none-any.whl size=1175239 sha256=69d9a44bc6220b6297048b4cce1b75716d287109912201bf457cee28295e09e4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-sosl9zdz/wheels/16/15/89/1c7bb31bd0006793a95549d04785121a8a36daad9158e1e43a\n",
            "Successfully built whisper\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers, ffmpeg-python, whisper\n",
            "Successfully installed ffmpeg-python-0.2.0 huggingface-hub-0.11.0 tokenizers-0.13.2 transformers-4.24.0 whisper-1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-2.5.1-py3-none-any.whl (15 kB)\n",
            "Collecting levenshtein==0.20.2\n",
            "  Downloading Levenshtein-0.20.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 44.6 MB/s \n",
            "\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.3.0\n",
            "  Downloading rapidfuzz-2.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 56.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, levenshtein, jiwer\n",
            "Successfully installed jiwer-2.5.1 levenshtein-0.20.2 rapidfuzz-2.13.2\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git \n",
        "!pip install jiwer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VsZ-Eq2uNFZ"
      },
      "source": [
        "Es posible que necesite instalar otros paquetes como rust o ffmpeg. Si lo solicita, ejecutando la siguiente linea de codigo se instalan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FC9KShcBuU2x"
      },
      "outputs": [],
      "source": [
        "# !pip install setuptools-rust\n",
        "# !pip install ffmpeg-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3eW6cBGuhm9"
      },
      "source": [
        "## Modelos e idiomas disponibles\n",
        "Hay cinco tamaños de modelos, cuatro con versiones solo en inglés, que ofrecen ventajas y desventajas de velocidad y precisión. A continuación se muestran los nombres de los modelos disponibles y sus requisitos aproximados de memoria y velocidad relativa.\n",
        "\n",
        "| Size\t| Parameters\t| English-only model\t| Multilingual model\t| Required VRAM\t| Relative speed |\n",
        "| :-----: | :-----: | :-----: | :-----: | :-----: | :-----: |\n",
        "|tiny\t|39 M\t|tiny.en\t|tiny\t|~1 GB\t|~32x |\n",
        "|base\t|74 M\t|base.en\t|base\t|~1 GB\t|~16x |\n",
        "|small\t|244 M\t|small.en\t|small\t|~2 GB\t|~6x |\n",
        "|medium\t|769 M\t|medium.en\t|medium\t|~5 GB\t|~2x |\n",
        "|large\t|1550 M\t|N/A\t|large\t|~10 GB\t|1x |\n",
        "\n",
        "Para aplicaciones solo en inglés, los modelos `.en`  tienden a funcionar mejor, especialmente para los modelos `tiny.en` y `base.en`. Se observa que la diferencia se vuelve menos significativa para los modelos `small.en` y `medium.en`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecCNiMfswBG5"
      },
      "source": [
        "## Uso en Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SJl7HJOeo0-P"
      },
      "outputs": [],
      "source": [
        "#@title Ejecutar esta celda para cargar la funcion para grabar audio.\n",
        "\n",
        "from IPython.display import HTML, Audio\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var my_div = document.createElement(\"DIV\");\n",
        "var my_p = document.createElement(\"P\");\n",
        "var my_btn = document.createElement(\"BUTTON\");\n",
        "var t = document.createTextNode(\"Presiona para comenzar a grabar\");\n",
        "\n",
        "my_btn.appendChild(t);\n",
        "//my_p.appendChild(my_btn);\n",
        "my_div.appendChild(my_btn);\n",
        "document.body.appendChild(my_div);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "var recordButton = my_btn;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "  gumStream = stream;\n",
        "  var options = {\n",
        "    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k\n",
        "    mimeType : 'audio/webm;codecs=opus'\n",
        "    //mimeType : 'audio/webm;codecs=pcm'\n",
        "  };            \n",
        "  //recorder = new MediaRecorder(stream, options);\n",
        "  recorder = new MediaRecorder(stream);\n",
        "  recorder.ondataavailable = function(e) {            \n",
        "    var url = URL.createObjectURL(e.data);\n",
        "    var preview = document.createElement('audio');\n",
        "    preview.controls = true;\n",
        "    preview.src = url;\n",
        "    document.body.appendChild(preview);\n",
        "\n",
        "    reader = new FileReader();\n",
        "    reader.readAsDataURL(e.data); \n",
        "    reader.onloadend = function() {\n",
        "      base64data = reader.result;\n",
        "      //console.log(\"Inside FileReader:\" + base64data);\n",
        "    }\n",
        "  };\n",
        "  recorder.start();\n",
        "  };\n",
        "\n",
        "recordButton.innerText = \"Grabando... presione para detener\";\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "\n",
        "function toggleRecording() {\n",
        "  if (recorder && recorder.state == \"recording\") {\n",
        "      recorder.stop();\n",
        "      gumStream.getAudioTracks()[0].stop();\n",
        "      recordButton.innerText = \"Guardando la grabación... por favor espere!\"\n",
        "  }\n",
        "}\n",
        "\n",
        "// https://stackoverflow.com/a/951057\n",
        "function sleep(ms) {\n",
        "  return new Promise(resolve => setTimeout(resolve, ms));\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "//recordButton.addEventListener(\"click\", toggleRecording);\n",
        "recordButton.onclick = ()=>{\n",
        "toggleRecording()\n",
        "\n",
        "sleep(2000).then(() => {\n",
        "  // wait 2000ms for the data to be available...\n",
        "  // ideally this should use something like await...\n",
        "  //console.log(\"Inside data:\" + base64data)\n",
        "  resolve(base64data.toString())\n",
        "\n",
        "});\n",
        "\n",
        "}\n",
        "});\n",
        "      \n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def get_audio():\n",
        "  display(HTML(AUDIO_HTML))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  \n",
        "  process = (ffmpeg\n",
        "    .input('pipe:0')\n",
        "    .output('pipe:1', format='wav')\n",
        "    .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n",
        "  )\n",
        "  output, err = process.communicate(input=binary)\n",
        "  \n",
        "  riff_chunk_size = len(output) - 8\n",
        "  # Break up the chunk size into four bytes, held in b.\n",
        "  q = riff_chunk_size\n",
        "  b = []\n",
        "  for i in range(4):\n",
        "      q, r = divmod(q, 256)\n",
        "      b.append(r)\n",
        "\n",
        "  # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.\n",
        "  riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "  sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "  return audio, sr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfucATJG7PNK"
      },
      "source": [
        "### Importamos las librerias necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMKezTHmvlwg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import whisper\n",
        "from scipy.io.wavfile import write\n",
        "from IPython.display import clear_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0mc7SrIy92m"
      },
      "source": [
        "### Grabar audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 96
        },
        "id": "opNkn_Lgpat4",
        "outputId": "e2f7fe09-78f9-43db-a3ce-29377425cd59"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<script>\n",
              "var my_div = document.createElement(\"DIV\");\n",
              "var my_p = document.createElement(\"P\");\n",
              "var my_btn = document.createElement(\"BUTTON\");\n",
              "var t = document.createTextNode(\"Presiona para comenzar a grabar\");\n",
              "\n",
              "my_btn.appendChild(t);\n",
              "//my_p.appendChild(my_btn);\n",
              "my_div.appendChild(my_btn);\n",
              "document.body.appendChild(my_div);\n",
              "\n",
              "var base64data = 0;\n",
              "var reader;\n",
              "var recorder, gumStream;\n",
              "var recordButton = my_btn;\n",
              "\n",
              "var handleSuccess = function(stream) {\n",
              "  gumStream = stream;\n",
              "  var options = {\n",
              "    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k\n",
              "    mimeType : 'audio/webm;codecs=opus'\n",
              "    //mimeType : 'audio/webm;codecs=pcm'\n",
              "  };            \n",
              "  //recorder = new MediaRecorder(stream, options);\n",
              "  recorder = new MediaRecorder(stream);\n",
              "  recorder.ondataavailable = function(e) {            \n",
              "    var url = URL.createObjectURL(e.data);\n",
              "    var preview = document.createElement('audio');\n",
              "    preview.controls = true;\n",
              "    preview.src = url;\n",
              "    document.body.appendChild(preview);\n",
              "\n",
              "    reader = new FileReader();\n",
              "    reader.readAsDataURL(e.data); \n",
              "    reader.onloadend = function() {\n",
              "      base64data = reader.result;\n",
              "      //console.log(\"Inside FileReader:\" + base64data);\n",
              "    }\n",
              "  };\n",
              "  recorder.start();\n",
              "  };\n",
              "\n",
              "recordButton.innerText = \"Grabando... presione para detener\";\n",
              "\n",
              "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
              "\n",
              "\n",
              "function toggleRecording() {\n",
              "  if (recorder && recorder.state == \"recording\") {\n",
              "      recorder.stop();\n",
              "      gumStream.getAudioTracks()[0].stop();\n",
              "      recordButton.innerText = \"Guardando la grabación... por favor espere!\"\n",
              "  }\n",
              "}\n",
              "\n",
              "// https://stackoverflow.com/a/951057\n",
              "function sleep(ms) {\n",
              "  return new Promise(resolve => setTimeout(resolve, ms));\n",
              "}\n",
              "\n",
              "var data = new Promise(resolve=>{\n",
              "//recordButton.addEventListener(\"click\", toggleRecording);\n",
              "recordButton.onclick = ()=>{\n",
              "toggleRecording()\n",
              "\n",
              "sleep(2000).then(() => {\n",
              "  // wait 2000ms for the data to be available...\n",
              "  // ideally this should use something like await...\n",
              "  //console.log(\"Inside data:\" + base64data)\n",
              "  resolve(base64data.toString())\n",
              "\n",
              "});\n",
              "\n",
              "}\n",
              "});\n",
              "      \n",
              "</script>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "audio, sr = get_audio()\n",
        "name = 'record.wav'\n",
        "write(name, sr, audio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0ePNjPc8nGY"
      },
      "source": [
        "### Aplicamos Whisper\n",
        "\n",
        "Como primer tarea vamos a tomar el audio que grabamos y transcribirlo.\n",
        "El metodo `transcribe()` se enacrga de transcribir un archivo de audio usando Whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDwpqSks8klP",
        "outputId": "b5948433-3ff8-41a6-a298-aafa4cfe4e4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- SU TRANSCRIPCIÓN --\n",
            "\n",
            " Solo transcribir audio a idioma original con whisper.\n",
            "\n",
            "Idioma detectado: es\n"
          ]
        }
      ],
      "source": [
        "model = whisper.load_model(\"medium\")\n",
        "result = model.transcribe(name)\n",
        "\n",
        "clear_output()\n",
        "print(\"-- SU TRANSCRIPCIÓN --\\n\")\n",
        "print('{}\\n'.format(result[\"text\"]))\n",
        "print(\"Idioma detectado: {}\".format(result['language']))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08TCumleC4gH"
      },
      "source": [
        "Si queremos guardar la transcripcion:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMTDM0-cBEGQ"
      },
      "outputs": [],
      "source": [
        "archivo_txt = name.split()[0] + '.txt'\n",
        "with open(archivo_txt, 'w', encoding=\"utf-8\") as t:\n",
        "    t.write(result[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwZDzQCJ_l-C"
      },
      "source": [
        "Internamente, el método `transcribe()` lee todo el archivo y procesa el audio con una ventana deslizante de 30 segundos, realizando predicciones autorregresivas de secuencia a secuencia en cada ventana."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71SjZuXtKX-n"
      },
      "source": [
        "## Uso a un nivel mas inferior\n",
        "Probemos ahora el uso de whisper.`detect_language()` y `whisper.decode()` que proporcionan acceso de nivel inferior al modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCY_Qe-t_ls9"
      },
      "outputs": [],
      "source": [
        "# cargar audio y rellenarlo/recortarlo para que se ajuste a 30 segundos\n",
        "audio = whisper.load_audio(name)\n",
        "audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "# creamos el espectrograma log-Mel y lo enviamos al dispositivo\n",
        "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5jxzCjLM5IK"
      },
      "source": [
        "Metodo `detect_language()` Detecta el idioma hablado en el audio y lo devuelve como una lista de strings, junto con los identificadores de los tokens de idioma más probables y la distribución de probabilidad sobre todos los tokens de idioma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EN5LSKCaLEOa",
        "outputId": "e42360a8-4961-4b21-8a1b-267b4dd085e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Idioma detectado: es\n"
          ]
        }
      ],
      "source": [
        "# detectar el idioma hablado\n",
        "_, probs = model.detect_language(mel)\n",
        "print(f\"Idioma detectado: {max(probs, key=probs.get)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_IsBe4GLNHS",
        "outputId": "b3076853-862e-4a6f-a36e-61c396821638"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Solo transcribir audio a idioma original con whisper.\n"
          ]
        }
      ],
      "source": [
        "# decodificar el audio\n",
        "options = whisper.DecodingOptions()\n",
        "result = whisper.decode(model, mel, options)\n",
        "\n",
        "# print the recognized text\n",
        "print(result.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0atARPKPcrz"
      },
      "source": [
        "## Traducir de español a ingles\n",
        "\n",
        "Para traducir usamos el metodo `transcribe()` aclarando que queremos que desarrolle la tarea de traduccion con:  `task=\"translate\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWMd8jd0LScz"
      },
      "outputs": [],
      "source": [
        "# Cargamos el modelo\n",
        "model = whisper.load_model(\"medium\")\n",
        "\n",
        "# Cargamos el audio\n",
        "audio = whisper.load_audio(name)\n",
        "\n",
        "# Le pasamos las tareas\n",
        "transcription = model.transcribe(audio)[\"text\"] # por defecto task='transcribe'\n",
        "translation = model.transcribe(audio, task=\"translate\")[\"text\"]\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEFUA7z3RmyM",
        "outputId": "90d2ed19-9130-4b71-94c8-70c59b1b08e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "transcription:  Solo transcribir audio a idioma original con whisper.\n",
            "translation:  Just transcribe audio to original language with whisper\n"
          ]
        }
      ],
      "source": [
        "print('transcription: {}'.format(transcription))\n",
        "print('translation: {}'.format(translation))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRyAZwk7SH9J"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "4bba7e2b24ea2d6c62673c3ed642d0129ead9d4c378b0ce2fb07275f742d82ac"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
